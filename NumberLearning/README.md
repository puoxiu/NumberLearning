# 机器学习
理解：
    通过各种学习算法，发现数据背后的规律，通过不断改进模型，作出预测。

    监督学习（有目标值）
        分类算法
        回归算法
    无监督学习（有目标值，从无标签的数据开始学习）
        聚类
# 0、基本问题

## 1、回归与分类

```c
回归: 是连续的，在数学上来说是给定一个点集，能够用一条曲线去拟合之，如果这个曲线是一条直线，那就被称为线性回归，如果曲线是一条二次曲线，就被称为二次回归。例如对房价的预测等；
分类：则是离散的，通过特征值计算与训练集距离预测其所处类别。例如对某个人工作类型的分类、本项目KNN数字识别也是分类。
```



## 2、矩阵求导

## 3、有监督学习和无监督学习的区别？

```c
有监督学习：
    对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。
    本项目：KNN算法，在对训练集训练时，将每个样本都归到一种数字上，对应数字就是标签
   	KNN,LR,SVM,BP,RF
无监督学习：
    对未标记的样本进行训练学习，比发现这些样本中的结构知识。
    对训练集训练时样本特征相近“自动分类”，但是不具备类标签。
    KMeans,DL
```

## 4、梯度下降

```c
使得代价或者损失最小的方法：
    梯度下降GD（Gradient Descent）：----以损失下降最快的方式 寻找代价（损失）最小的过程
        按学习率前进（学习率：下降的速率？）
    小批量梯度下降--每次选择部分样本计算出学习率
    随机梯度下降
梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向（牛顿方向）。
以二维平面来说，则求导求当前点的切线方向。
    
```

## 5、最小二乘法

```c
最小二乘法--基于均方误差（欧氏距离）进行模型求解的方法：
    即求预测值与真实值差的平方和，力图使得误差达到最小；
    在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小
```

## 6、K-means算法

```c
将事先输入的n个数据对象划分为k个聚类以便使得所获得的聚类相似度较高，而不同聚类中的对象相似度较小；让每个聚类内的点尽量紧密的连在一起，而让聚类之间的距离尽量的大。
算法描述：
（1）适当选择c个类的初始中心；
（2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的那个中心所在的类；
（3）利用均值等方法更新该类的中心值；
（4）对于所有的C个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束；否则继续迭代。
```

## 7、标准化与归一化

```c
无量纲化
```



# 1. 特征工程

## 特征提取

### 数据预处理
无量纲化处理
    归一化：通过函数变换将原始数据映射到（0,1）区间
        

    标准化：

## 特征降维

降低随机变量（特征）个数，得到一组“不相关”的主变量的过程---降低特征之间的相关性（很强的相关性对算法的预测有较大的影响）
降维对象：二维数组

### 两种降维方式：
    特征选择---找出样本主要特征
        过滤式 
            低方差特征过滤
    
        Embedded嵌入式
    
    主成分分析--PCA
    定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
    作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
    应用：回归分析或者聚类分析当中

# 2. 转换器与估计器
    是sklearn的机器学习算法实现---算法的API



# 3. 分类

## 3.1 K-临近算法 KNN

    常用来分类，可分类也可回归，分类--取前K个最相似的（距离最小）数据作为同一个分类
    k-临近邻算法采用测量不同特征值之间的距离方法进行分类。
    距离公式---欧式距离（差的平方和）
    优点：简单，无需训练
    缺点：懒惰算法，内存开销大 易受K值的影响，需要不断测试
    适用于小数据场景
一般流程：


## 3.2 模型选择与调优
    交叉验证：将训练集再分为训练和验证集
    网格搜索：
        评估每组超参数，得出最优组合
        超参数：需手动指定的参数，如K-临近算法中的K值

## 3.3 使用jumpyter noterbook
    安装： sudo pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter
    进入：jupyter notebook

## 3.4 朴素贝叶斯算法
朴素:指的就是条件独立，特征之间相互独立

即在特征之间相互独立的情况下，以贝叶斯公式算法计算（后验）概率。

```markdown
条件概率: P(A|B) = P(A*B) / P(B)
若A与B相互独立，则有：P（A*B） = P（A） * P（B）
贝叶斯公式：P(A|B) = P(A) * P(B|A) / P(B)

先验概率
条件概率
后验概率
```
拉普拉斯平滑
应用:文档分类



## 决策树
基尼系数：用于计算一个系统中的失序现象。
    基尼系数越高，越混乱
另一种衡量系统混乱程度的经典手段：
    信息熵：H(x) = - 求和 p(i) * log (p(i))
    单位：bit
    条件熵：在某个条件下（或者说分类之后）的信息熵
决策树划分依据--对应不同的算法：
    ID3：熵增益：条件熵与原本信息熵的差，值越大说明在该分类后的混乱程度越低
    C4/5：熵增益比
    CART：基尼系数最小准则
优点:
    简单\可视化
缺点:
    过拟合
改进:
    剪枝cart算法
    随机森林

## 随机森林
集成学习方法:
    建立多个分类器/模型,各自独立训练模型,组合预测,因此优于单一预测
随机森林:

# 拟合
欠拟合
过拟合

    正则化-Regularization:  
        减少泛化误差，而不是减少训练误差-----减少机器学习过拟合的过程
        L1范数
        L2范数





# 回归
连续型
## 线性回归 Linear regression
回归方程：特征值与目标值建立的一个关系
最小二乘法--基于均方误差（欧氏距离）进行模型求解的方法
    在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小

损失函数-代价函数：

使得损失最小：
    正归方程：求导得来
    梯度下降GD（Gradient Descent）：----以损失下降最快的方式 寻找代价（损失）最小的过程
        定义代价函数
        选择起始点
        计算梯度
        按学习率前进（学习率：下降的速率？）
    小批量梯度下降--每次选择部分样本计算出学习率

均方误差的评估：
    MSE

## 岭回归---带有L2正则化的线性回归
    解决过拟合问题

## 逻辑回归
    分类算法--解决二分类问题
    sigmoid函数


# 无监督学习

## K-means算法



